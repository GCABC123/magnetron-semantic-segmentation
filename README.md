 ğŸ¤– THE ABC 123 GROUP â„¢ ğŸ¤–

ğŸŒ GENERAL CONSULTING ABC 123 BY OSAROPRIME â„¢.

ğŸŒ ABC 123 USA â„¢

ğŸŒ ABC 123 DESYGN â„¢

ğŸŒ ABC 123 FILMS â„¢

=============================================================

                     ğŸŒ MAGENTRON â„¢ ğŸŒ
                     
ğŸŒ ARTIFICIAL INTELLIGENCE 2.0 â„¢ : OBJECT MASKING PROXIA A-2 (SEMANTIC SEGMENTATION). (SENSE: VISION_EYE CAMERAS)

*ï¸âƒ£ğŸ“¶ğŸ¤–

- PHYSICAL WORLD SENSE: SIGHT âœ…

- PHYSICAL WORLD SENSE: SMELL 

- PHYSICAL WORLD SENSE: HEARING 

- PHYSICAL WORLD SENSE: TASTE

- PHYSICAL WORLD SENSE: TOUCH

+++++++++++++++++++++++++++++++++++++

ğŸŒ ASTRAL BODY MINDCLOUD: NO

ğŸŒ PRANIC BODY MINDCLOUD: NO

ğŸŒ INSTINCTIVE MIND MINDCLOUD: âœ…

ğŸŒ ASTRAL MIND MINDCLOUD: NO

ğŸŒ PRANIC MIND MINDCLOUD: NO

REQUIREMENTS: 

[*] Software Requirements: Google Colab/Jupyter Notebook, Python

[*] HARDWARE REQUIREMENTS: fast TPU/GPU.

[*] DEPENDENCIES: INCLUDED


=============================================================

This is a Google Colab/Jupyter Notebook for developing (one possible scheme for) a MASKING PROXIA when working with ARTIFICIAL INTELLIGENCE 2.0 â„¢ 
(ARTIFICIAL INTELLIGENCE 2.0â„¢ is part of MAGNETRON â„¢ TECHNOLOGY). The machine running the Notebook will be a MINDCLOUD on which you will be
developing a PROXIA to detect objects in IMAGES (for example: pictures from social media platforms).

e.g This INSTINCTIVE MIND MINDCLOUD  PROXIA can be used to process INFORMATION from the real world via the eye cameras and then send information about the objects detected to the IMAGINATION proxia on ASTRAL MINDCLOUD for the robot to IMAGINE it in different scenarios to better understand what it is and how people see it. So for example if the ROBOT detects a cup with the OBJECT DETECTION on the INSTINCTIVE MIND PROXIA it can imagine a cup in some typical or unusual scenarios (On ASTRAL PROXIA) to better understand what it is and how humans see it.

Prerequisite reading:

- ARTIFICIAL INTELLIGENCE PRIMER â„¢: https://www.facebook.com/artificialintelligenceprimer

- ARTIFICIAL INTELLIGENCE 2.0 â„¢ DOCUMENTATION: https://www.facebook.com/aibyabc123/

- MEMBERS CLUB â„¢ DOCUMENTATION: https://www.facebook.com/abc123membersclub/

THERE ARE 2 MAIN TYPES OF SEGMENTATION (ALSO KNOWN AS MASKING) EXIST:

#SEMANTIC SEGMENTATION

With Semantic segmentation objects shown in an image are grouped based on defined categories. For instance, a street scene would be segmented by â€œpedestrians,â€ â€œbikes,â€ â€œvehicles,â€ â€œsidewalks,â€ and so on.


#INSTANCE SEGMENTATION

Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image. This is different from SEMANTIC SEGMENTATION. Semantic segmentation associates every pixel of an image with a class label such as a person, flower, car and so on. It treats multiple objects of the same class as a single entity. In contrast, instance segmentation treats multiple objects of the same class as distinct individual instances. Consider instance segmentation a refined version of semantic segmentation. Categories like â€œvehiclesâ€ are split into â€œcars,â€ â€œmotorcycles,â€ â€œbuses,â€ and so on â€” instance segmentation detects the instances of each category.


#DIFFERENCE BETWEEN SEMANTIC SEGMENTATION AND INSTANCE SEGMENTATION

Semantic segmentation treats multiple objects within a single category as one entity. Instance segmentation, on the other hand, identifies individual objects within these categories. 



ğŸ‘‘ 
INCLUDED STICKERS/SIGN:

FIND STICKERS HERE: https://bit.ly/3B8D3lE

- PROMOTIONAL MATERIAL FOR ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢. (CUSTOM GRAPHICS BY ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ——ğ—˜ğ—¦ğ—¬ğ—šğ—¡ â„¢/ğ—¢ğ—¦ğ—”ğ—¥ğ—¢ ğ—›ğ—”ğ—¥ğ—¥ğ—œğ—¢ğ—§ğ—§). THE ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢  SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ FOR ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢. ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢.

*ï¸âƒ£ğŸ“¶ğŸ¤–

- PROMOTIONAL MATERIAL FOR ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢. (CUSTOM GRAPHICS BY ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ——ğ—˜ğ—¦ğ—¬ğ—šğ—¡ â„¢/ğ—¢ğ—¦ğ—”ğ—¥ğ—¢ ğ—›ğ—”ğ—¥ğ—¥ğ—œğ—¢ğ—§ğ—§) THE ğ——ğ—¥ğ—”ğ—šğ—¢ğ—¡ & ğ—–ğ—¥ğ—¢ğ—ªğ—¡ ğŸ‘‘ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ ASSOCIATED WITH TECHNOLOGY. ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢.

You must display the included stickers/signs (so that it is clearly visible) if you are working with MAGNETRON â„¢ TECHNOLOGY for the purposes of determining whether you want to purchase a technology license or not. This includes but is not limited to public technology displays, trade shows, technology expos, media appearances, Investor events, Computers (exterior), MINDCLOUD STORAGE (e.g server room doors, render farm room doors) etc.


NOTE: SEE ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ DOCUMENTATION FOR INFORMATION ABOUT THE MAIN MASKING PROXIA (ON INSTINCTIVE MIND MINDCLOUD).

NOTE: CLICK HERE FOR A NOTEBOOK ON MAKING MASKING PROXIA WITH INSTANCE SEGMENTATION (INSTEAD OF SEMANTIC SEGMENTATION): 




### [YouTube](https://youtu.be/odAGA7pFBGA)  | [Cityscapes Score](https://www.cityscapes-dataset.com/method-details/?submissionID=7836) <br>
<br>


## Installation 

* The code is tested with pytorch 1.3 and python 3.6
* You can use ./Dockerfile to build an image.


## Download Weights

* Create a directory where you can keep large files. Ideally, not in this directory.
```bash
  > mkdir <large_asset_dir>
```

* Update `__C.ASSETS_PATH` in `config.py` to point at that directory

  __C.ASSETS_PATH=<large_asset_dir>

* Download pretrained weights from [google drive](https://drive.google.com/open?id=1fs-uLzXvmsISbS635eRZCc5uzQdBIZ_U) and put into `<large_asset_dir>/seg_weights`

## Download/Prepare Data

If using Cityscapes, download Cityscapes data, then update `config.py` to set the path:
```python
__C.DATASET.CITYSCAPES_DIR=<path_to_cityscapes>
```

* Download Autolabelled-Data from [google drive](https://drive.google.com/file/d/1DtPo-WP-hjaOwsbj6ZxTtOo_7R_4TKRG/view?usp=sharing)

If using Cityscapes Autolabelled Images, download Cityscapes data, then update `config.py` to set the path:
```python
__C.DATASET.CITYSCAPES_CUSTOMCOARSE=<path_to_cityscapes>
```

If using Mapillary, download Mapillary data, then update `config.py` to set the path:
```python
__C.DATASET.MAPILLARY_DIR=<path_to_mapillary>
```


## Running the code

The instructions below make use of a tool called `runx`, which we find useful to help automate experiment running and summarization. For more information about this tool, please see [runx](https://github.com/NVIDIA/runx).
In general, you can either use the runx-style commandlines shown below. Or you can call `python train.py <args ...>` directly if you like.


### Run inference on Cityscapes

Dry run:
```bash
> python -m runx.runx scripts/eval_cityscapes.yml -i -n
```
This will just print out the command but not run. It's a good way to inspect the commandline. 

Real run:
```bash
> python -m runx.runx scripts/eval_cityscapes.yml -i
```

The reported IOU should be 86.92. This evaluates with scales of 0.5, 1.0. and 2.0. You will find evaluation results in ./logs/eval_cityscapes/...

### Run inference on Mapillary

```bash
> python -m runx.runx scripts/eval_mapillary.yml -i
```

The reported IOU should be 61.05. Note that this must be run on a 32GB node and the use of 'O3' mode for amp is critical in order to avoid GPU out of memory. Results in logs/eval_mapillary/...

### Dump images for Cityscapes

```bash
> python -m runx.runx scripts/dump_cityscapes.yml -i
```

This will dump network output and composited images from running evaluation with the Cityscapes validation set. 

### Run inference and dump images on a folder of images

```bash
> python -m runx.runx scripts/dump_folder.yml -i
```

You should end up seeing images that look like the following:

![alt text](imgs/composited_sf.png "example inference, composited")

## Train a model

Train cityscapes, using HRNet + OCR + multi-scale attention with fine data and mapillary-pretrained model
```bash
> python -m runx.runx scripts/train_cityscapes.yml -i
```

The first time this command is run, a centroid file has to be built for the dataset. It'll take about 10 minutes. The centroid file is used during training to know how to sample from the dataset in a class-uniform way.

This training run should deliver a model that achieves 84.7 IOU.

## Train SOTA default train-val split
```bash
> python -m runx.runx  scripts/train_cityscapes_sota.yml -i
```
Again, use `-n` to do a dry run and just print out the command. This should result in a model with 86.8 IOU. If you run out of memory, try to lower the crop size or turn off rmi_loss.
